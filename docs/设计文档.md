# capstan设计文档

此项目名暂定为capstan(轮船的绞盘)。

## conception

![](docs/images/concept.png)

## 测试指标

### 集群指标

1. 集群创建时间

2. k8s的各api延迟

3. pod启动延迟

### node节点指标

1. pod启动延迟

2. 每个业务场景下的cpu利用率（包括空集群）

3. 每个业务场景下的memory利用率（包括空集群）

4. 磁盘I/O

5. 网络I/O

### 业务场景测试指标

依据论文Workload Classification Model for Specializing Virtual Machine Operating System和Workload Classification & Software Energy Measurement for Efficient Scheduling on Private Cloud Platforms中提到的四种分类进行划分。

#### CPU-Intensive

大部分时间用来做计算和逻辑判断等CPU动作的程序称之为CPU-Intensive。例如计算圆周率，在执行过程中绝大部分时间用在三角函数和开根号的计算，便是CPU-Intensive程序。

我们可以用以下benchmark来衡量：

1. FFT (Mflops)--进行傅里叶变化时，每秒浮点操作的个数

#### Memory-Intensive

需要内存存储大量的计算数据来不停地进行数据的迭代计算操作的程序为Memory-Intensive。例如SVM支持向量机的训练，预测过程。

1. memory-bandwith

#### I/O-Intensive

可以使用fio工具或者数据库的性能测试工具来测试或者表征磁盘I/O方面的性能。

fio工具可以测试磁盘，ssd的IO性能（可以模拟日志，数据库，流处理等应用）主要有如下指标：

sequential_write（顺序写），sequential_read（顺序读），random_write（随机写），random_read（随机读）等以下指标

1. iops ()--每秒进行(I/O)操作的次数
2. bandwidth (KB/s)--带宽
3. latency_min (usec)--最小延迟
4. atency_max (usec)--最大延迟
5. latency_mean (usec)--平均延迟
6. atency_p50 (usec)--50%操作的延迟
7. latency_p90 (usec)--90%操作的延迟
8. latency_p99 (usec)--99%操作的延迟
9. latency_p99.9 (usec)--99.9%操作的延迟

mongodb，radis，cassandra等数据库都有相应的性能测试工具，数据库性能的主要指标如下：

1. overall RunTime (ms)--总共运行的时间
2. overall Throughput (ops/sec)--总共的吞吐量
3. insert MinLatency (ms)--插入操作的最小延迟
4. insert MaxLatency (ms)--插入操作的最大延迟
5. insert p50 latency (ms)--50%插入操作的延迟
6. insert p75 latency (ms)--75%插入操作的延迟
7. insert p90 latency (ms)--90%插入操作的延迟
8. insert p95 latency (ms)--95%插入操作的延迟
9. insert p99 latency (ms)--99%插入操作的延迟
10. insert p99.9 latency (ms)--99.9%插入操作的延迟
11. read MinLatency (ms)--读操作的最小延迟
12. read MaxLatency (ms)--读操作的最大延迟
13. read p50 latency (ms)--50%读操作的延迟
14. read p75 latency (ms)--75%读操作的延迟
15. read p90 latency (ms)--90%读操作的延迟
16. read p95 latency (ms)--95%读操作的延迟
17. read p99 latency (ms)--99%读操作的延迟
18. read p99.9 latency (ms)--99.9%读操作的延迟
19. update MinLatency (ms)--更新操作的最小延迟
20. update MaxLatency (ms)--更新操作的最大延迟
21. update p50 latency (ms)--50%更新操作的延迟
22. update p75 latency (ms)--75%更新操作的延迟
23. update p90 latency (ms)--90%更新操作的延迟
24. update p95 latency (ms)--95%更新操作的延迟
25. update p99 latency (ms)--99%更新操作的延迟
26. update p99.9 latency (ms)--99.9%更新操作的延迟

#### Network-Intensive

可以使用iperf，netperf，ping等工具测试网络方面的性能。

主要有以下指标：

1. TCP_Throughput (Mbits/sec)--tcp吞吐量

2. TCP_RR_Average_Latency (ms)--在一次TCP连接中，client端和server端的transaction平均延迟

3. TCP_STREAM_Total_Throughput (Mbits/sec)--TCP流式通讯时的吞吐量

4. 延迟时间

同时也可以使用wrk，ab等工具测试nginx，tomcat等http服务器的性能。

主要有如下指标：

1. bytes transferred (bytes)--传输的总字节数
2. requests ()--请求次数
3. throughput (requests/sec)--请求的吞吐量
4. Average response time (ms)--平均响应时间
5. p50 latency (ms)--50%请求的延迟
6. p75 latency (ms)--75%请求的延迟
7. p90 latency (ms)--90%请求的延迟
8. p95 latency (ms)--95%请求的延迟
9. p99 latency (ms)--99%请求的延迟
10. p99.9 latency (ms)--99.9%请求的延迟

### k8s集群组件指标

主要组件包括如下两类：

1. kubelet

因为kubelet组件是以二进制形式启动的，所以可通过top，pmap等命令，或者读取proc文件等方式获取kubelet的cpu，内存的使用情况。

2. apiserver,kube-controller-manager,kube-proxy以及云平台厂商提供的系统容器如ingress-controller，fluentd-log，api-gateway等

因为以上组件基本是以容器化方式运行的，可以直接通过cadvisor直接获取cpu，内存的使用情况。

主要指标如下：

1. cpu最大使用量
2. cpu最小使用量
3. cpu平均使用量
4. 50%时间cpu使用量
5. 70%时间cpu使用量
6. 90%时间cpu使用量
7. 内存最大使用量
8. 内存最小使用量
9. 内存平均使用量
10. 50%时间内存使用量
11. 70%时间内存使用量
12. 90%时间内存使用量

## 架构

主要包括两个组件capstan(轮船的绞盘)，propeller(轮船的螺旋桨)

![](docs/images/capstan.png)

### capstan

主要功能：

1. 集群生命周期管理。借助于kops在aliyun，gcp，aws等云服务提供商上创建指定配置的k8s集群，测试完成清除所创建的k8s集群。集群配置参数可通过配置文件指定。（同时也支持在不是capstan所管理的集群进行workload测试，参数为`"CloudProvider": "local"`，后续测试流程一样）

2. 启动propeller服务。capstan根据capstan配置文件生成的propeller的配置文件创建propeller服务。

3. 数据收集。propeller服务所产生的workload的测试数据，k8s组件的性能数据以及k8s组件发送到capstan进行持久化存储。

4. 数据展示。对收集到的数据进行统计分析并展示。

### propeller

主要功能：

1. workload测试。遍历需要测试的workload。根据每一个workload下指定的test case set，依此准备环境测试每一个test case，test case测试完毕后清理环境然后测试下一个test case。

2. 数据收集。收集每一个test case的测试结果以及k8s集群组件在每一个test case下的性能指标（主要是cpu，内存的指标）。

### 流程

1. capstan利用kops在云服务商aliyun，gcp，aws创建指定配置的k8s集群。

2. capstan通过k8s api创建propeller服务。

3. propeller根据配置文件依次启动workload并测试每一个test case。

4. 收集每一个test case的数据以及k8s组件的性能数据。

5. 根据配置文件的`"CapstanEndpoint": "user_machine_ip:8081"`选项导出数据到capstan中进行持久化存储，数据分析以及展示。

### capstan配置文件

```json
{
    "Description": "capstan",
    "Clusters": {
        "CloudProvider": "Aliyun", //在aliyun上搭建集群，支持Aliyun，gcp，aws，local等参数
        "K8sVersion": "1.9",       //搭建k8s集群的版本
        "NodeNums": "10",          //k8s集群节点数
        "OS": "ubuntu16.04",       //k8s集群操作系统
        "VCPU": "4",               //节点vcpu核数
        "Memeroy": "8G"            //节点内存大小
    },
    "Propeller": {          //propeller配置部分
        "Description": "propeller",
        "ResultsDir": "/tmp/propeller", //测试数据存储地址
        "CapstanEndpoint": "user_machine_ip:8081", //capstan的endpoint，数据导出到此地址
        "Steps": 10,  //每个workload之间的测试时间间隔
        "Sampling": 5,  //k8s集群组件性能采样时间间隔
        "Workload": [
            {
                "name": "http-test", //名为http-test的测试
                "type": "wrk", //所用的测试工具
                "image": "nginx:17.9", //workload的镜像
                "Steps": 5, //每个test case之间的测试时间间隔
                "testCaseSet": { //test case测试集
                    "benchmarkPodIPSameNode": true,
                    "benchmarkVIPSameNode": true,
                    "benchmarkPodIPDiffNode": true,
                    "benchmarkVIPDiffNode": true
                }
            },
            {
                "name": "network-test1",
                "type": "iperf",
                "Steps": 5,
                "testCaseSet": {
                    "benchmarkTCPSameNode": true,
                    "benchmarkUDPSameNode": true,
                    "benchmarkTCPDiffNode": true,
                    "benchmarkUDPDiffNode": true
                }
            }
        ]
    },
    "ResultsDir": "/etc/capstan", //capstan数据持久化存储的地址
    "Server": { //capstan数据展示的http服务
        "advertiseaddress": "user_machine_ip:8080",
        "bindaddress": "0.0.0.0",
        "bindport": 8080,
        "timeoutseconds": 5400
    },
    "Version": "v0.1" //版本信息
}
```

### propeller配置文件

```json
{
    "Description": "propeller",
    "ResultsDir": "/tmp/propeller",
    "CapstanEndpoint": "user_machine_ip:8081",
    "Steps": 10,
    "Sampling": 5,
    "Workload": [
        {
            "name": "http-test",
            "type": "wrk",
            "image": "nginx:17.9",
            "Steps": 5,
            "testCaseSet": {
                "benchmarkPodIPSameNode": true,
                "benchmarkVIPSameNode": true,
                "benchmarkPodIPDiffNode": true,
                "benchmarkVIPDiffNode": true
            }
        },
        {
            "name": "network-test1",
            "type": "iperf",
            "Steps": 5,
            "testCaseSet": {
                "benchmarkTCPSameNode": true,
                "benchmarkUDPSameNode": true,
                "benchmarkTCPDiffNode": true,
                "benchmarkUDPDiffNode": true
            }
        }
    ]
}
```

### propeller服务的yaml文件

```yaml
apiVersion: v1 //新建一个namespace
kind: Namespace
metadata:
  name: propeller
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: propeller
  name: propeller-serviceaccount
  namespace: propeller
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    component: propeller
  name: propeller-serviceaccount
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: propeller-serviceaccount
subjects:
- kind: ServiceAccount
  name: propeller-serviceaccount
  namespace: propeller
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    component: propeller
  name: propeller-serviceaccount
  namespace: propeller
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
---
apiVersion: v1 //
data:
  config.json: |
    {
        "Description": "propeller",
        "ResultsDir": "/tmp/propeller",
        "Steps": 10,
        "Sampling": 5,
        "CapstanEndpoint": "user_machine_ip:8081",
        "Workload": [
          {
            "name": "http-test",
            "type": "wrk",
            "image": "nginx:17.9",
            "Steps": 5,
            "testCaseSet": {
              "benchmarkPodIPSameNode": true,
              "benchmarkVIPSameNode": true,
              "benchmarkPodIPDiffNode": true,
              "benchmarkVIPDiffNode": true
            }
          },
          {
            "name": "network-test1",
            "type": "iperf",
            "Steps": 5,
            "testCaseSet": {
              "benchmarkTCPSameNode": true,
              "benchmarkUDPSameNode": true,
              "benchmarkTCPDiffNode": true,
              "benchmarkUDPDiffNode": true
            }
          }
        ]
    }
kind: ConfigMap
metadata:
  labels:
    component: propeller
  name: propeller-config-cm
  namespace: propeller
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    component: propeller
  name: propeller
  namespace: propeller
spec:
  containers:
  - command:
    - /bin/bash
    - -c
    - /propeller --no-exit=true -v 3 --logtostderr
    env:
    - name: PROPELLER_ADVERTISE_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    image: wadelee/propeller:v0.1
    imagePullPolicy: IfNotPresent
    name: propeller
    volumeMounts:
    - mountPath: /etc/propeller
      name: propeller-config-volume
    - mountPath: /tmp/propeller
      name: output-volume
  restartPolicy: Never
  serviceAccountName: propeller-serviceaccount
  volumes:
  - configMap:
      name: propeller-config-cm
    name: propeller-config-volume
  - emptyDir: {}
    name: output-volume
---
apiVersion: v1
kind: Service
metadata:
  labels:
    component: propeller
  name: propeller
  namespace: propeller
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    component: propeller
  type: ClusterIP
---
```

## 项目目录结构

```text
├── Makefile
├── README.md
├── cmd
│   ├── capstan
│   │   └── capstan.go //capstan程序入口
│   └── propeller
│       └── propeller.go //propeller程序入口
├── docs            //文档目录
│   ├── README.md
│   ├── configuration.md //程序配置详细说明
│   ├── design.md //设计文档
│   ├── developer.md //开发者文档
│   └── images
│       ├── capstan.png
│       └── concept.png
├── examples
│   ├── capstan_conf.json //capstan配置example
│   ├── propeller.yaml //propeller服务的k8s yaml文件example
│   └── propeller_conf.json //propeller配置example
├── hack //脚本工具
├── main.go
├── pkg //项目具体的功能实现
│   ├── analysis //数据分析包
│   │   ├── analysis.go //接口文件
│   │   ├── iperf.go //iperf工具产生的数据的分析代码处，实现analysis的接口
│   │   ├── tensorflow.go //tensorflow工具产生的数据的分析代码处，实现analysis的接口
│   │   └── wrk.go //wrk工具产生的数据的分析代码处，实现analysis的接口
│   ├── build //工具的编译文件包
│   │   ├── iperf //iperf镜像需要的所有文件包
│   │   │   ├── Dockerfile
│   │   │   ├── Makefile
│   │   │   └── run_iperf.sh
│   │   ├── tensorflow
│   │   └── wrk
│   ├── config
│   │   └── config.go //项目的配置文件解析处
│   ├── dashboard
│   │   └── dashboard.go //数据显示代码实现处
|   ├── data //组件性能数据采集
|   │   ├── cadvisor
|   │   │   └── cadvisor.go //通过cadvisor采集容器的性能数据
|   │   ├── kubelet
|   │   │   └── kubelet.go //kubelet组件性能数据采集的代码实现处
|   │   └── testtoollog
|   │       └── testtoollog.go //测试工具测试结果数据收集代码实现处
│   ├── driver //propeller调用k8s的驱动
│   │   ├── deamonset.go //deamonset驱动
│   │   ├── deployment.go //deployment驱动
│   │   └── job.go //job驱动
│   ├── kops
│   │   ├── aliyun.go //aliyun实现集群生命周期管理接口的代码处
│   │   ├── aws.go //aws实现集群生命周期管理接口的代码处
│   │   ├── gcp.go //gcp实现集群生命周期管理接口的代码处
│   │   └── kops.go //集群生命周期管理的接口
│   ├── testtools
│   │   ├── testtools.go //测试工具的接口
│   │   ├── iperf
│   │   │   └── iperf.go //iperf测试工具的代码实现处
│   │   ├── tensorflow
│   │   │   └── tensorflow.go //tensorflow测试工具的代码实现处
│   │   └── wrk
│   │       └── wrk.go //wrk测试工具的代码实现处
│   ├── utils
│   │   └── k8s_util
│   │       └── k8s_util.go
│   └── workload
│       ├── nginx.go //workload为nginx的代码实现处
│       ├── tensorflow.go //workload为tensorflow的代码实现处
│       └── workload.go //workload接口
└── tools //项目脚本工具
```

## 开发者(每新增一个workload如何操作)

### 新增的一个workload有相应的测试工具

1. 只需要实现pkg/workload下workload.go的接口

### 新增的一个workload没有相应的测试工具

1. 需要先实现测试工具相关的接口，主要在pkg/testtools下的testtool.go以及pkg/analysis下的analysis.go。

2. 然后需要实现pkg/workload下workload.go的接口
